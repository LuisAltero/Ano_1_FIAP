hive> CREATE TABLE tabela_empregado (cd_empregado STRING, nm_empregado STRING, ds_cargo STRING, cd_gerente STRING, dt_contratacao STRING, vl_salario STRING, vl_comissao STRING, cd_depto STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' TBLPROPERTIES("skip.header.line.count"="1");
OK
Time taken: 0.17 seconds
hive> SHOW COLUMNS FROM tabela_empregado;
OK
cd_empregado        
nm_empregado        
ds_cargo            
cd_gerente          
dt_contratacao      
vl_salario          
vl_comissao         
cd_depto            
Time taken: 0.116 seconds, Fetched: 8 row(s)
hive> SELECT * FROM tabela_empregado;
OK
Time taken: 0.803 seconds
hive> show tables;
OK
tabela_empregado
Time taken: 0.065 seconds, Fetched: 1 row(s)
hive> DROP TABLE tabela_empregado;
OK
Time taken: 0.227 seconds
hive> CREATE TABLE EMP (cd_empregado STRING, nm_empregado STRING, ds_cargo STRING, cd_gerente STRING, dt_contratacao STRING, vl_salario STRING, vl_comissao STRING, cd_depto STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' TBLPROPERTIES("skip.header.line.count"="1");
OK
Time taken: 0.165 seconds
hive> show tables;
OK
emp
Time taken: 0.036 seconds, Fetched: 1 row(s)
hive> show columns from emp;
OK
cd_empregado        
nm_empregado        
ds_cargo            
cd_gerente          
dt_contratacao      
vl_salario          
vl_comissao         
cd_depto            
Time taken: 0.122 seconds, Fetched: 8 row(s)
hive> DROP TABLE emp;
OK
Time taken: 0.238 seconds
hive> CREATE TABLE EMP (cd_empregado INT, nm_empregado STRING, ds_cargo STRING, cd_gerente INT, dt_contratacao DATE, vl_salario FLOAT, vl_comissao FLOAT, cd_depto INT) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' TBLPROPERTIES("skip.header.line.count"="1");
OK
Time taken: 0.159 seconds
hive> LOAD DATA LOCAL INPATH '/home/oracle/Downloads/empregados.csv' OVERWRITE INTO TABLE EMP;
Loading data to table database_empregado.emp
OK
Time taken: 0.82 seconds
hive> 
    > SELECT nm_empregado, ds_cargo, cd_gerente FROM emp;
OK
MILTON	PROFESSOR	7902
PAULO	ADMINISTRATIVO	7698
NEUZA	ADMINISTRATIVO	7698
HUMBERTO	COORDENADOR	7839
NEUZA	ADMINISTRATIVO	7698
ALLEN	COORDENADOR	7839
HENRIQUE	COORDENADOR	7839
RITA	COORDENADOR	7566
VAGNER	DIRETOR	
THAIS	ADMINISTRATIVO	7698
OLIVIA	PROFESSOR	7788
PATRICIA	PROFESSOR	7698
FABIO	GERENTE	7566
RENATO	PROFESSOR	7782
NULL	NULL	NULL
Time taken: 0.205 seconds, Fetched: 15 row(s)
hive> DROP TABLE emp;
OK
Time taken: 0.214 seconds
hive> CREATE TABLE EMP (cd_empregado STRING, nm_empregado STRING, ds_cargo STRING, cd_gerente STRING, dt_contratacao STRING, vl_salario STRING, vl_comissao STRING, cd_depto STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' TBLPROPERTIES("skip.header.line.count"="1");
OK
Time taken: 0.188 seconds
hive> LOAD DATA LOCAL INPATH '/home/oracle/Downloads/empregados.csv' OVERWRITE INTO TABLE EMP;
Loading data to table database_empregado.emp
OK
Time taken: 0.275 seconds
hive> CREATE TABLE emp STORED AS ORC
    >   AS SELECT CAST(cd_empregado AS INT) cd_empregado, nm_empregado, ds_cargo, 
    >             CAST(cd_gerente AS INT) cd_gerente,
    > TO_DATE(FROM_UNIXTIME(UNIX_TIMESTAMP(contratado,'dd/MM/yyyy'))) contratado,
    >             CAST(vl_salario AS DECIMAL (7,2)) vl_salario,
    >             CAST(vl_comissao AS DECIMAL (7,2)) vl_comissao,
    >             CAST(cd_depto AS INT) cd_depto
    >   FROM emp;
FAILED: SemanticException org.apache.hadoop.hive.ql.parse.SemanticException: Table already exists: database_empregado.emp
hive> CREATE TABLE emps STORED AS ORC
    >   AS SELECT CAST(cd_empregado AS INT) cd_empregado, nm_empregado, ds_cargo, 
    >             CAST(cd_gerente AS INT) cd_gerente,
    > TO_DATE(FROM_UNIXTIME(UNIX_TIMESTAMP(contratado,'dd/MM/yyyy'))) contratado,
    >             CAST(vl_salario AS DECIMAL (7,2)) vl_salario,
    >             CAST(vl_comissao AS DECIMAL (7,2)) vl_comissao,
    >             CAST(cd_depto AS INT) cd_depto
    >   FROM emp;
FAILED: SemanticException [Error 10004]: Line 4:37 Invalid table alias or column reference 'contratado': (possible column names are: cd_empregado, nm_empregado, ds_cargo, cd_gerente, dt_contratacao, vl_salario, vl_comissao, cd_depto)
hive> CREATE TABLE emps STORED AS ORC
    >   AS SELECT CAST(cd_empregado AS INT) cd_empregado, nm_empregado, ds_cargo, 
    >             CAST(cd_gerente AS INT) cd_gerente,
    > TO_DATE(FROM_UNIXTIME(UNIX_TIMESTAMP(dt_contratacao,'dd/MM/yyyy'))) dt_contratacao,
    >             CAST(vl_salario AS DECIMAL (7,2)) vl_salario,
    >             CAST(vl_comissao AS DECIMAL (7,2)) vl_comissao,
    >             CAST(cd_depto AS INT) cd_depto
    >   FROM emp;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1619434888566_0005, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0005/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2021-08-31 08:08:03,998 Stage-1 map = 0%,  reduce = 0%
2021-08-31 08:08:16,058 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 7.21 sec
MapReduce Total cumulative CPU time: 7 seconds 210 msec
Ended Job = job_1619434888566_0005
Stage-3 is selected by condition resolver.
Stage-2 is filtered out by condition resolver.
Stage-4 is filtered out by condition resolver.
Moving data to: hdfs://bigdatalite.localdomain:8020/user/hive/warehouse/database_empregado.db/.hive-staging_hive_2021-08-31_08-07-46_048_7578271658798180478-1/-ext-10001
Moving data to: hdfs://bigdatalite.localdomain:8020/user/hive/warehouse/database_empregado.db/emps
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1   Cumulative CPU: 7.21 sec   HDFS Read: 5427 HDFS Write: 1324 SUCCESS
Total MapReduce CPU Time Spent: 7 seconds 210 msec
OK
Time taken: 31.422 seconds
hive> SELECT nm_empregado, vl_salario, dt_contratacao FROM emps;
OK
MILTON	800	2010-12-17
PAULO	1600	2011-02-20
NEUZA	1250	2011-02-22
HUMBERTO	2975	2011-04-02
NEUZA	1250	2011-09-28
ALLEN	2850	2011-05-01
HENRIQUE	2450	2011-06-06
RITA	3000	2017-04-19
VAGNER	5000	2011-11-17
THAIS	1500	2011-08-08
OLIVIA	1100	2017-05-23
PATRICIA	950	0178-08-03
FABIO	3000	2011-11-03
RENATO	1300	2012-01-21
NULL	NULL	NULL
Time taken: 0.163 seconds, Fetched: 15 row(s)
hive> 
    > SELECT nm_empregado, vl_salario, ds_cargo FROM emps WHERE vl_salario > 2850;
OK
HUMBERTO	2975	COORDENADOR
RITA	3000	COORDENADOR
VAGNER	5000	DIRETOR
FABIO	3000	GERENTE
Time taken: 0.325 seconds, Fetched: 4 row(s)
hive> SELECT concat(nm_empregado, “tem o cargo de“, ds_cargo) AS alias FROM EMP;
NoViableAltException(26@[147:1: selectExpression : ( expression | tableAllColumns );])
	at org.antlr.runtime.DFA.noViableAlt(DFA.java:158)
	at org.antlr.runtime.DFA.predict(DFA.java:116)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectExpression(HiveParser_SelectClauseParser.java:4216)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectExpression(HiveParser.java:44540)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.function(HiveParser_IdentifiersParser.java:4583)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.atomExpression(HiveParser_IdentifiersParser.java:6758)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceFieldExpression(HiveParser_IdentifiersParser.java:6861)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnaryPrefixExpression(HiveParser_IdentifiersParser.java:7246)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceUnarySuffixExpression(HiveParser_IdentifiersParser.java:7306)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseXorExpression(HiveParser_IdentifiersParser.java:7490)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceStarExpression(HiveParser_IdentifiersParser.java:7650)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedencePlusExpression(HiveParser_IdentifiersParser.java:7810)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAmpersandExpression(HiveParser_IdentifiersParser.java:7970)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceBitwiseOrExpression(HiveParser_IdentifiersParser.java:8129)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceEqualExpression(HiveParser_IdentifiersParser.java:8659)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceNotExpression(HiveParser_IdentifiersParser.java:9672)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceAndExpression(HiveParser_IdentifiersParser.java:9791)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.precedenceOrExpression(HiveParser_IdentifiersParser.java:9950)
	at org.apache.hadoop.hive.ql.parse.HiveParser_IdentifiersParser.expression(HiveParser_IdentifiersParser.java:6566)
	at org.apache.hadoop.hive.ql.parse.HiveParser.expression(HiveParser.java:44583)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectItem(HiveParser_SelectClauseParser.java:3066)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectList(HiveParser_SelectClauseParser.java:1326)
	at org.apache.hadoop.hive.ql.parse.HiveParser_SelectClauseParser.selectClause(HiveParser_SelectClauseParser.java:1089)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectClause(HiveParser.java:44573)
	at org.apache.hadoop.hive.ql.parse.HiveParser.singleSelectStatement(HiveParser.java:41669)
	at org.apache.hadoop.hive.ql.parse.HiveParser.selectStatement(HiveParser.java:41375)
	at org.apache.hadoop.hive.ql.parse.HiveParser.regularBody(HiveParser.java:41312)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpressionBody(HiveParser.java:40365)
	at org.apache.hadoop.hive.ql.parse.HiveParser.queryStatementExpression(HiveParser.java:40241)
	at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:1525)
	at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1061)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:201)
	at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:423)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:311)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1194)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1289)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1120)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1108)
	at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:218)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:170)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:381)
	at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:773)
	at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:691)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:626)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.util.RunJar.run(RunJar.java:221)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:136)
FAILED: ParseException line 1:33 cannot recognize input near 'tem' 'o' 'cargo' in select expression
hive> SELECT concat(nm_empregado, 'tem o cargo de', ds_cargo) AS alias FROM EMP;
OK
MILTONtem o cargo dePROFESSOR
PAULOtem o cargo deADMINISTRATIVO
NEUZAtem o cargo deADMINISTRATIVO
HUMBERTOtem o cargo deCOORDENADOR
NEUZAtem o cargo deADMINISTRATIVO
ALLENtem o cargo deCOORDENADOR
HENRIQUEtem o cargo deCOORDENADOR
RITAtem o cargo deCOORDENADOR
VAGNERtem o cargo deDIRETOR
THAIStem o cargo deADMINISTRATIVO
OLIVIAtem o cargo dePROFESSOR
PATRICIAtem o cargo dePROFESSOR
FABIOtem o cargo deGERENTE
RENATOtem o cargo dePROFESSOR
NULL
Time taken: 0.166 seconds, Fetched: 15 row(s)
hive> SELECT nm_empregado, vl_salario FROM emps WHERE vl_salario >= 1000 and vl_salario <= 3000;
OK
PAULO	1600
NEUZA	1250
HUMBERTO	2975
NEUZA	1250
ALLEN	2850
HENRIQUE	2450
RITA	3000
THAIS	1500
OLIVIA	1100
FABIO	3000
RENATO	1300
Time taken: 0.404 seconds, Fetched: 11 row(s)
hive> 
    > SELECT nm_empregado, ds_cargo, vl_salario FROM emps WHERE vl_salario >= 1500 and vl_salario <= 5000 and ds_cargo = 'COORDENADOR' or ds_cargo = 'ADMINISTRATIVO';
OK
PAULO	ADMINISTRATIVO	1600
NEUZA	ADMINISTRATIVO	1250
HUMBERTO	COORDENADOR	2975
NEUZA	ADMINISTRATIVO	1250
ALLEN	COORDENADOR	2850
HENRIQUE	COORDENADOR	2450
RITA	COORDENADOR	3000
THAIS	ADMINISTRATIVO	1500
Time taken: 0.267 seconds, Fetched: 8 row(s)
hive> SELECT COUNT(*) FROM emps;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1619434888566_0006, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0006/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-08-31 08:44:32,872 Stage-1 map = 0%,  reduce = 0%
2021-08-31 08:44:43,869 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.65 sec
2021-08-31 08:44:54,641 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.62 sec
MapReduce Total cumulative CPU time: 8 seconds 620 msec
Ended Job = job_1619434888566_0006
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 8.62 sec   HDFS Read: 8655 HDFS Write: 3 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 620 msec
OK
15
Time taken: 39.25 seconds, Fetched: 1 row(s)
hive> SELECT ds_cargo, COUNT(*) TOTAL
    >   FROM emps
    >  GROUP BY ds_cargo;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1619434888566_0007, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0007/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-08-31 08:45:53,455 Stage-1 map = 0%,  reduce = 0%
2021-08-31 08:46:03,740 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.18 sec
2021-08-31 08:46:15,592 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.12 sec
MapReduce Total cumulative CPU time: 9 seconds 120 msec
Ended Job = job_1619434888566_0007
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 9.12 sec   HDFS Read: 9137 HDFS Write: 68 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 120 msec
OK
NULL	1
ADMINISTRATIVO	4
COORDENADOR	4
DIRETOR	1
GERENTE	1
PROFESSOR	4
Time taken: 37.138 seconds, Fetched: 6 row(s)
hive> SELECT MIN(vl_salario) FROM EMPS;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1619434888566_0008, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0008/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0008
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-08-31 08:47:21,553 Stage-1 map = 0%,  reduce = 0%
2021-08-31 08:47:31,569 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.82 sec
2021-08-31 08:47:42,255 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.51 sec
MapReduce Total cumulative CPU time: 8 seconds 510 msec
Ended Job = job_1619434888566_0008
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 8.51 sec   HDFS Read: 8944 HDFS Write: 4 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 510 msec
OK
800
Time taken: 35.624 seconds, Fetched: 1 row(s)
hive> SELECT MAX(vl_salario) FROM EMPS;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1619434888566_0009, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0009/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0009
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-08-31 08:48:34,287 Stage-1 map = 0%,  reduce = 0%
2021-08-31 08:48:44,251 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.51 sec
2021-08-31 08:48:54,868 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.47 sec
MapReduce Total cumulative CPU time: 8 seconds 470 msec
Ended Job = job_1619434888566_0009
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 8.47 sec   HDFS Read: 8952 HDFS Write: 5 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 470 msec
OK
5000
Time taken: 35.42 seconds, Fetched: 1 row(s)
hive> SELECT SUM(campo) FROM EMPS;
FAILED: SemanticException [Error 10004]: Line 1:11 Invalid table alias or column reference 'campo': (possible column names are: cd_empregado, nm_empregado, ds_cargo, cd_gerente, dt_contratacao, vl_salario, vl_comissao, cd_depto)
hive> SELECT SUM(vl_salario) FROM EMPS;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1619434888566_0010, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0010/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0010
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-08-31 08:50:45,953 Stage-1 map = 0%,  reduce = 0%
2021-08-31 08:50:55,980 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.62 sec
2021-08-31 08:51:05,550 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.25 sec
MapReduce Total cumulative CPU time: 8 seconds 250 msec
Ended Job = job_1619434888566_0010
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 8.25 sec   HDFS Read: 8965 HDFS Write: 6 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 250 msec
OK
29025
Time taken: 36.457 seconds, Fetched: 1 row(s)
hive> SELECT AVG(vl_salario) FROM EMPS;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1619434888566_0011, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0011/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0011
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-08-31 08:51:51,177 Stage-1 map = 0%,  reduce = 0%
2021-08-31 08:52:01,170 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.69 sec
2021-08-31 08:52:11,862 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.42 sec
MapReduce Total cumulative CPU time: 8 seconds 420 msec
Ended Job = job_1619434888566_0011
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 8.42 sec   HDFS Read: 9351 HDFS Write: 12 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 420 msec
OK
2073.214286
Time taken: 36.889 seconds, Fetched: 1 row(s)
hive> SELECT STDDEV(vl_salario) FROM EMPS;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1619434888566_0012, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0012/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0012
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-08-31 08:53:00,849 Stage-1 map = 0%,  reduce = 0%
2021-08-31 08:53:10,862 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.73 sec
2021-08-31 08:53:21,506 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 9.08 sec
MapReduce Total cumulative CPU time: 9 seconds 80 msec
Ended Job = job_1619434888566_0012
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 9.08 sec   HDFS Read: 9269 HDFS Write: 19 SUCCESS
Total MapReduce CPU Time Spent: 9 seconds 80 msec
OK
1139.4886182952814
Time taken: 35.559 seconds, Fetched: 1 row(s)
hive> SELECT ds_cargo, COUNT(nm_emp) TOTAL FROM emps GROUP BY ds_cargo;
FAILED: SemanticException [Error 10004]: Line 1:23 Invalid table alias or column reference 'nm_emp': (possible column names are: cd_empregado, nm_empregado, ds_cargo, cd_gerente, dt_contratacao, vl_salario, vl_comissao, cd_depto)
hive> SELECT ds_cargo, COUNT(nm_empregado) TOTAL FROM emps GROUP BY ds_cargo;
Query ID = oracle_20210831072020_b0d3fd36-baa6-4338-b4cd-66d0e9bc8cc8
Total jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1619434888566_0013, Tracking URL = http://bigdatalite.localdomain:8088/proxy/application_1619434888566_0013/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1619434888566_0013
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2021-08-31 09:03:23,674 Stage-1 map = 0%,  reduce = 0%
2021-08-31 09:03:33,377 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.77 sec
2021-08-31 09:03:43,988 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.47 sec
MapReduce Total cumulative CPU time: 8 seconds 470 msec
Ended Job = job_1619434888566_0013
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 8.47 sec   HDFS Read: 9404 HDFS Write: 68 SUCCESS
Total MapReduce CPU Time Spent: 8 seconds 470 msec
OK
NULL	0
ADMINISTRATIVO	4
COORDENADOR	4
DIRETOR	1
GERENTE	1
PROFESSOR	4
Time taken: 36.954 seconds, Fetched: 6 row(s)
hive> 
